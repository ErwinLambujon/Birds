# -*- coding: utf-8 -*-
"""Copy of Reporting Clustering.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Xcw3ygaHBawMaNBTocGfAkyZ4frN33CY
"""

import tensorflow as tf
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import joblib

from sklearn.preprocessing import RobustScaler
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import confusion_matrix
from scipy import stats
import seaborn as sns

from tensorflow import keras
Sequential = tf.keras.models.Sequential

Bidirectional = tf.keras.layers.Bidirectional
LSTM = tf.keras.layers.LSTM
Dropout = tf.keras.layers.Dropout
Dense = tf.keras.layers.Dense

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import tensorflow as tf
from tensorflow import keras
import pandas as pd
import seaborn as sns
from pylab import rcParams
import matplotlib.pyplot as plt
from matplotlib import rc
from pandas.plotting import register_matplotlib_converters

# %matplotlib inline
# %config InlineBackend.figure_format='retina'

register_matplotlib_converters()
sns.set(style='whitegrid', palette='muted', font_scale=1.5)

rcParams['figure.figsize'] = 22, 10

RANDOM_SEED = 42

np.random.seed(RANDOM_SEED)
tf.random.set_seed(RANDOM_SEED)

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv(
  '/content/drive/MyDrive/Colab Notebooks/Datasets/bird.csv'
)

df.shape
df = df.dropna()

df.head(100)

df.shape

# Assuming df is your DataFrame
described = df.describe()
for col in described.columns:
    described[col] = described[col].apply(int)
described

df.nunique()

unique_activities = df['type'].unique()
print(unique_activities)

"""Each bird has a label for its ecological group:




*   SW: Swimming Birds
*   W: Wading Birds
*   T: Terrestrial Birds
*   R: Raptors
*   P: Scansorial Birds
*   SO: Singing Birds











"""

# Counting the occurrences of each Bird types
type_counts = df['type'].value_counts()

# Plotting the distribution of activities
plt.figure(figsize=(10, 6))
type_counts.plot(kind='bar', color='teal')
plt.title('Distribution of Bird Types')
plt.xlabel('Bird Types')
plt.ylabel('Frequency')
plt.xticks(rotation=45)
plt.show()

def plot_activity(type, df):
    data = df[df['type'] == type][['huml', 'humw', 'ulnal', 'ulnaw', 'feml', 'femw', 'tibl', 'tibw', 'tarl', 'tarw']][:200]
    axis = data.plot(subplots=True, figsize=(16, 12),
                     title=type)
    for ax in axis:
        ax.legend(loc='lower left', bbox_to_anchor=(1.0, 0.5))

plot_activity("T", df);

plot_activity("P", df);

plot_activity("SO", df);

plot_activity("SW", df);

plot_activity("W", df);

plot_activity("R", df);

#needs refactoring

# Check if the 'activity' column in the DataFrame 'df' is of type 'object'
if df['type'].dtype == 'object':
    # If true, convert the 'activity' column to a categorical type
    # This is done to save memory and improve performance for columns with a limited number of unique values
    # The 'astype('category')' method changes the data type of the 'activity' column to 'category'
    # The 'cat.codes' attribute then converts the categorical data into an array of codes, which are integer representations of the categories
    df['type'] = df['type'].astype('category').cat.codes

# Display the first few rows of the DataFrame to verify the changes
df.head(200)

df.head(400)

type_counts

unique_activities = df['type'].unique()
print(unique_activities)

df = df.sort_values(by=['id', 'type'])

# Create a new column 'activity_shifted' to hold the previous row's activity value
df['type_shifted'] = df['type'].shift(1)

# Create a new column 'user_shifted' to hold the previous row's user_id value
df['id_shifted'] = df['id'].shift(1)

# Identify changes in activity or user_id
df['type_change'] = (df['type'] != df['type_shifted']) | (df['id'] != df['id_shifted'])

# Cumulatively sum the changes to create a group identifier for each sequence of the same activity
df['type_block'] = df['type_change'].cumsum()

# Count the number of entries in each activity block
type_counts = df.groupby(['id', 'type', 'type_block']).size().reset_index(name='count')

# Drop the 'activity_block' as it's no longer needed beyond counting
type_counts = type_counts.drop(columns='type_block')

# Show the DataFrame
df.to_csv('dataframe.csv', index=False)
type_counts.to_csv('acounts.csv', index=False)

sns.countplot(x = 'id',
              data = df,
              palette=[sns.color_palette()[0]],
              order = df.id.value_counts().index);
plt.title("Records per user");

df_train = df[df['id'] <= 300]
df_test = df[df['id'] > 300]

from sklearn.preprocessing import RobustScaler

scale_columns = ['huml', 'humw', 'ulnal', 'ulnaw', 'feml', 'femw', 'tibl', 'tibw', 'tarl', 'tarw']

scaler = RobustScaler()
scaler.fit(df_train[scale_columns])



df_train.loc[:, scale_columns] = scaler.transform(df_train[scale_columns].to_numpy())
df_test.loc[:, scale_columns] = scaler.transform(df_test[scale_columns].to_numpy())

described = df_train.describe()
for col in described.columns:
    described[col] = described[col].apply(int)
described

df.head()

def create_dataset(df, time_steps):
    Xs, ys = [], []
    # Iterate over each unique block identified by user_id and activity_block
    for _, group in df.groupby(['id', 'type_block']):
        # Check the number of full batches in the group
        full_batches = len(group) // time_steps
        if full_batches == 0:
            continue  # Skip this block if there aren't enough data points

        # Process each full batch
        for i in range(full_batches):
            start_idx = i * time_steps
            end_idx = start_idx + time_steps
            v = group.iloc[start_idx:end_idx][['huml', 'humw', 'ulnal', 'ulnaw', 'feml', 'femw', 'tibl', 'tibw', 'tarl', 'tarw']].values  # Extract the features

            # Since all entries in a block are guaranteed to be the same activity, we can take the first one
            type_label = group['type'].iloc[start_idx]

            Xs.append(v)
            ys.append(type_label)

    return np.array(Xs), np.array(ys).reshape(-1, 1)

from sklearn.model_selection import train_test_split

TIME_STEPS = 1

X, y = create_dataset(df, TIME_STEPS)

# Evenly distributing the data based on activity count
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

print(X_train.shape, y_train.shape)
print(X_test.shape, y_test.shape)

enc = OneHotEncoder(handle_unknown='ignore', sparse_output=False)

enc = enc.fit(y_train)

y_train_encoded = enc.transform(y_train)
y_test_encoded = enc.transform(y_test)

print("One-Hot Encoded y_train:\n", y_train_encoded)
print("One-Hot Encoded y_test:\n", y_test_encoded)

ReduceLROnPlateau = tf.keras.callbacks.ReduceLROnPlateau
EarlyStopping = tf.keras.callbacks.EarlyStopping
Conv1D =  tf.keras.layers.Conv1D
MaxPooling1D =  tf.keras.layers.MaxPooling1D

model = Sequential()

# 1D Convolutional layer for feature extraction
model.add(Conv1D(filters=64, kernel_size=3, activation='relu', padding='same', input_shape=[X_train.shape[1], X_train.shape[2]]))
model.add(MaxPooling1D(pool_size=1))

model.add(Bidirectional(
    LSTM(units=128, return_sequences=True)
    # input_shape=[X_train.shape[1], X_train.shape[2]]
))

model.add(Bidirectional(
    LSTM(units=128)
))

model.add(Dropout(rate=0.5))
model.add(Dense(units=128, activation='relu'))
model.add(Dense(units=64, activation='relu'))
model.add(Dense(units=32, activation='relu'))
model.add(Dense(y_train_encoded.shape[1], activation='softmax'))

model.compile(
    loss='categorical_crossentropy',
    optimizer='adam',
    metrics=['acc']
)
# Dynamically adjust the learning rate during training to prevent getting stuck in local minima
# Configure the ReduceLROnPlateau callback
reduce_lr = ReduceLROnPlateau(
    monitor='val_loss',   # It keeps track of a specified metric  during training epochs.
    factor=0.1,
    patience=10,         #  If the metric doesn't improve for a certain number of epochs learning rate will be reduced
    min_lr=0.0001,       # Lower bound on the learning rate
    verbose=1
)
# Prevent training from continuing unnecessarily when the model isn't improving on the validation se
# Configure the EarlyStopping callback
early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=15,         # Number of epochs with no improvement after which learning rate will be reduced
    restore_best_weights=True,
    verbose=1
)

history = model.fit(
    X_train, y_train_encoded,
    epochs=100,
    batch_size=64,
    validation_split=0.1,
    callbacks=[reduce_lr, early_stopping],  # Add callbacks here
    verbose=1,
    shuffle=False
)

model.evaluate(X_test, y_test_encoded)

y_pred = model.predict(X_test)

loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, len(loss) + 1)

plt.figure(figsize=(10, 6))
plt.plot(epochs, loss, 'bo-', label='Training loss')  # 'bo-' gives blue color with dot and solid line
plt.plot(epochs, val_loss, 'ro-', label='Validation loss')  # 'ro-' gives red color with dot and solid line
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

"""SW: Swimming Birds
W: Wading Birds
T: Terrestrial Birds
R: Raptors
P: Scansorial Birds
SO: Singing Birds
"""

classes = ['SW', 'W', 'T', 'R', 'P', 'SO']

y_pred_labels = np.argmax(y_pred, axis=1)
y_test_labels = np.argmax(y_test_encoded, axis=1)

# Compute confusion matrix
cm = confusion_matrix(y_test_labels, y_pred_labels)

# Plot the confusion matrix
plt.figure(figsize=(10, 7))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
        xticklabels=classes, yticklabels=classes
    )
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
joblib.dump(model, 'birds_classification.pkl')

file_path = 'birds_classification.pkl'

with open(file_path, 'rb') as file:
    model = joblib.load(file)

print(model)

"""The model perfomed well in identifying the Terrestrial Birds and the Raptors"""
